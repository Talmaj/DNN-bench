version: 2.1

commands: # a reusable command with parameters
  benchmark:
    parameters:
      backend:
        type: string
      quantize:
        type: string
    steps:
      - checkout
      - run:
          command: >
            apt-get update &&
            apt-get -y install curl &&
            curl -o mobilenetv2-1.0.onnx
            https://s3.amazonaws.com/onnx-model-zoo/mobilenet/mobilenetv2-1.0/mobilenetv2-1.0.onnx
          name: Download model
      - run:
          command: >
            python3 ./bench mobilenetv2-1.0.onnx
            --backend <<parameters.backend>>
            --repeat 10
            --number 1
            --warmup 1
            --backend-meta <<parameters.backend>>
            --device cpu
            --quantize <<parameters.quantize>>
            --output-path results/mobilenet.json
          name: Benchmark pytorch backend
      - run: test results/mobilenet.json

jobs:
  bench-pytorch-cpu:
    docker:
      - image: toriml/pytorch:latest
    parameters:
      quantize:
        default: "0"
        type: string
    steps:
      - benchmark:
          backend: "pytorch"
          quantize: <<parameters.quantize>>

  bench-onnxruntime-openmp-cpu:
    docker:
      - image: toriml/onnxruntime:latest
    parameters:
      quantize:
        default: "0"
        type: string
    steps:
      - benchmark:
          backend: "onnxruntime"
          quantize: <<parameters.quantize>>

  bench-onnxruntime-openvino-cpu:
    docker:
      - image: mcr.microsoft.com/azureml/onnxruntime:latest-openvino-cpu
    parameters:
      quantize:
        default: "0"
        type: string
    steps:
      - benchmark:
          backend: "onnxruntime"
          quantize: <<parameters.quantize>>

  bench-onnxruntime-nuphar-cpu:
    docker:
      - image: mcr.microsoft.com/azureml/onnxruntime:latest-nuphar
    parameters:
      quantize:
        default: "0"
        type: string
    steps:
      - benchmark:
          backend: "onnxruntime"
          quantize: <<parameters.quantize>>

  bench-tensorflow-cpu:
    docker:
      - image: toriml/tensorflow:latest
    parameters:
      quantize:
        default: "0"
        type: string
    steps:
      - benchmark:
          backend: "tf"
          quantize: <<parameters.quantize>>

workflows:
  main:
    jobs:
      - bench-pytorch-cpu:
          matrix:
            parameters:
              quantize: ["0", "1"]
      - bench-onnxruntime-openmp-cpu:
          matrix:
            parameters:
              quantize: [ "0", "1" ]
      - bench-onnxruntime-openvino-cpu:
          matrix:
            parameters:
              quantize: [ "0", "1" ]
      - bench-onnxruntime-nuphar-cpu:
          matrix:
            parameters:
              quantize: [ "0", "1" ]
      - bench-tensorflow-cpu

